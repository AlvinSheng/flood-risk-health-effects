---
title: "Analysis before fitting the CAR model"
author: "Alvin Sheng"
date: "6/28/2021"
output: pdf_document
---

```{r}
library(here)
library(ape)
library(GGally)
library(usdm)
library(spdep)
library(factoextra)
library(tidyverse)
library(performance)
```

```{r}
fhs_model_df <- readRDS(here("intermediary_data/fhs_model_df_all_census_tract_reorg.rds"))
```

# Summary Statistics for Table 1 of paper

```{r}
first_var <- 19

summ_stats <- round(t(apply(fhs_model_df[, first_var:ncol(fhs_model_df)], 2, function(vec) {
  c(mean(vec, na.rm = T), sd(vec, na.rm = T), range(vec, na.rm = T))
})),5)

colnames(summ_stats) <- c("mean", "sd", "min", "max")

summ_stats
```

# Checking for multicollinearity among the covariates

`S.CARleroux()` automatically puts a fixed ridge penalty on the beta coefficients. Therefore, the large number of covariates and multicollinearity would be accounted for. 

Actually no, because the penalty is negligible. 

## Flood risk variables

```{r}
fr_index <- 14:35
```

```{r}
ggcorr(data = fhs_model_df[, c(fr_index, ncol(fhs_model_df))])
```

```{r}
flood_cor <- cor(fhs_model_df[complete.cases(fhs_model_df[, c(fr_index, ncol(fhs_model_df))]), c(fr_index, ncol(fhs_model_df))])

flood_cor[nrow(flood_cor), ] # correlation with dependent variable
```

For each variable, I take the summary of its correlations with other variables, not including itself. 

```{r}
diag(flood_cor) <- NA

summary(flood_cor)
```

Many of the flood risk variables are very correlated.



# Using VIF to exclude variables

```{r}
fhs_model_df <- readRDS(here("intermediary_data/fhs_model_df_all_census_tract_reorg.rds"))
```

```{r}
X <- fhs_model_df[, 14:(ncol(fhs_model_df) - 4)]

X <- X[, names(X) != "pct_floodfactor1"]

X <- X[, names(X) != "avg_risk_score_sfha"]



X           <- scale(X) # Scale covariates

X <- data.frame(X)
```

```{r}
vif(X)
```

```{r}
vifstep(X)
```

This procedure detects that the following variables have collinearity problems. Let's exclude these variables and then rerun the analysis. 

```{r}
collin_var_names <- c("avg_risk_score_all", "pct_fs_risk_2050_500", "pct_fs_risk_2020_500", "avg_risk_fsf_2020_500", "pct_fs_risk_2050_5", "pct_fs_risk_2020_100", "no2", "pct_fs_risk_2050_100")
```



# Correlations among climate related variables: flood risk, pollution, and GRIDMET variables

Excluding variables in collin_var_names

```{r}
climate_var_idx <- c(fr_index, 52:61)

climate_var_idx_exclude <- climate_var_idx[-which(names(fhs_model_df)[climate_var_idx] %in% collin_var_names)]
```

```{r}
ggcorr(data = fhs_model_df[, c(climate_var_idx_exclude, ncol(fhs_model_df))])
```

```{r}
climate_cor <- cor(fhs_model_df[complete.cases(fhs_model_df[, c(climate_var_idx_exclude, ncol(fhs_model_df))]), c(climate_var_idx_exclude, ncol(fhs_model_df))])

climate_cor[nrow(climate_cor), ] # correlation with dependent variable
```

For each variable, I take the summary of its correlations with other variables, not including itself. 

```{r}
diag(climate_cor) <- NA

summary(climate_cor)
```

Climate variables other than flood risk are not too correlated.




# Non-spatial modeling

```{r}
Y <- fhs_model_df$Data_Value_CHD

X <- fhs_model_df[, 14:(ncol(fhs_model_df) - 4)]

X <- X[, names(X) != "pct_floodfactor1"]

# exclude some more variables selected by vifstep, to account for multicollinearity
# excluding all of the pct_fs_risk variables, as well as 3 of the avg_risk_score variables

collin_var_names <- c("avg_risk_score_all", "pct_fs_risk_2050_500", "pct_fs_risk_2020_500", "avg_risk_fsf_2020_500", "pct_fs_risk_2050_5", "pct_fs_risk_2020_100", "no2", "pct_fs_risk_2050_100")

X <- X[, !(names(X) %in% collin_var_names)]

# also removing avg_risk_score_sfha due to large numbers of NAs
X <- X[, names(X) != "avg_risk_score_sfha"]



X           <- scale(X) # Scale covariates
X[is.na(X)] <- 0        # Fill in missing values with the mean

# if I do mean imputation (which may be problematic), all the counties 
# will have neighbors in W

# X <- data.frame(X)
```

```{r}
fhs_lm <- lm(Y ~ X)
```

```{r}
summary(fhs_lm)
```



# Checking for spatial autocorrelation

<!-- Adjust data due to missingness, to get code to work -->

```{r}
W <- readRDS(here("intermediary_data", "census_tract_adj_reorganize_all_census_tract.rds"))

W_listw <- mat2listw(W)
```

Moran's I

```{r}
(moran_results <- moran.test(residuals(fhs_lm), W_listw))
```

The *p*-value is negligible, so we can reject the null hypothesis of zero spatial autocorrelation. Since the observed value of I is significantly greater then the expected value, the life expectancies are positively autocorrelated, in contrast to negatively autocorrelated. Thus, using a CAR model is justified. 



# PCA

Conduct PCA on the correlated flood risk variables

```{r}
fr_index <- 14:35

# omitting the 24th variable, avg_risk_score_sfha, because of too many NAs
flood_risk <- fhs_model_df[, fr_index] %>% dplyr::select(-avg_risk_score_sfha)
```

```{r}
fr_pca <- prcomp(flood_risk[complete.cases(flood_risk),], center = T, scale. = T)

fr_loadings <- fr_pca$rotation
```

```{r}
fviz_eig(fr_pca)
```

```{r}
summ_pca <- summary(fr_pca)

summ_pca$importance[,1:10]
```

We started out with 22 variables. Including four PC scores would include 80% of the variance.

Including seven PC scores would include 90% of the variance.



Printing out the loadings, from most negative to least

```{r}
# First PC Score
fr_loadings[order(fr_loadings[, 1]), 1]
```

The first PC score is very interpretable. Only the loading for pct_floodfactor1 is positive.

```{r}
# Second PC Score
fr_loadings[order(fr_loadings[, 2]), 2]
```

Less interpretable--more of a mix of positive and negative loadings.

```{r}
# Third PC Score
fr_loadings[order(fr_loadings[, 3]), 3]
```

```{r}
# Fourth PC Score
fr_loadings[order(fr_loadings[, 4]), 4]
```

```{r}
# Fourth PC Score
fr_loadings[order(fr_loadings[, 5]), 5]
```



# Checking the distributions of the count variables ahead of using Poisson CAR

```{r}
fhs_model_df <- readRDS(here("intermediary_data/fhs_model_df_all_census_tract_pc.rds"))
```

Convert the health outcomes into counts

```{r}
fhs_model_df_counts <- fhs_model_df %>% 
  mutate(Data_Value_CHD = round(Data_Value_CHD / 100 * TotalPopulation), 
         Data_Value_BPHIGH = round(Data_Value_BPHIGH / 100 * TotalPopulation), 
         Data_Value_CASTHMA = round(Data_Value_CASTHMA / 100 * TotalPopulation), 
         Data_Value_MHLTH = round(Data_Value_MHLTH / 100 * TotalPopulation))
```

```{r}
hist(fhs_model_df_counts$Data_Value_CHD)
hist(fhs_model_df_counts$Data_Value_BPHIGH)
hist(fhs_model_df_counts$Data_Value_CASTHMA)
hist(fhs_model_df_counts$Data_Value_MHLTH)
```

For convenience, I will fit a simple non-spatial glm to the data, and analyze whether the residuals indicate overdispersion. 

I can then fit a CARBayes model and test for overdispersion on its residuals. 

```{r}
dat_CHD <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 46, 47, 48)]

glm_CHD <- glm(Data_Value_CHD ~ ., family = poisson, offset = log(fhs_model_df_counts$TotalPopulation), 
               data = dat_CHD)

check_overdispersion(glm_CHD)
```

What if I applied the overdispersion test just to the residuals? Use the code shown in Gelman's textbook

```{r}

n <- stats::nobs(glm_CHD)

k <- length(insight::find_parameters(glm_CHD, effects = "fixed", flatten = TRUE))

yhat <- predict(glm_CHD, type="response")
z <- (dat_CHD$Data_Value_CHD[complete.cases(dat_CHD)] - yhat)/sqrt(yhat)
cat ("overdispersion ratio is ", sum(z^2) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")

```



```{r}
dat_BPHIGH <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 46, 48)]

glm_BPHIGH <- glm(Data_Value_BPHIGH ~ ., family = poisson, offset = log(fhs_model_df_counts$TotalPopulation), 
                  data = dat_BPHIGH)

check_overdispersion(glm_BPHIGH)
```

```{r}
dat_CASTHMA <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 47, 48)]

glm_CASTHMA <- glm(Data_Value_CASTHMA ~ ., family = poisson, offset = log(fhs_model_df_counts$TotalPopulation), 
                   data = dat_CASTHMA)

check_overdispersion(glm_CASTHMA)
```

```{r}
dat_MHLTH <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 46, 47)]

glm_MHLTH <- glm(Data_Value_MHLTH ~ ., family = poisson, offset = log(fhs_model_df_counts$TotalPopulation), 
                 data = dat_MHLTH)

check_overdispersion(glm_MHLTH)
```

What if I applied the overdispersion test just to the residuals? Use the code shown in Gelman's textbook

```{r}

n <- stats::nobs(glm_MHLTH)

k <- length(insight::find_parameters(glm_MHLTH, effects = "fixed", flatten = TRUE))

yhat <- predict(glm_MHLTH, type="response")
z <- (dat_MHLTH$Data_Value_MHLTH[complete.cases(dat_MHLTH)] - yhat)/sqrt(yhat)
cat ("overdispersion ratio is ", sum(z^2) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")

```

Overdispersion detected in all cases.



# Residual Diagnostics with Gaussian CAR models

Just apply the overdispersion check to the residuals of the Gaussian models, to see if you get similar results

```{r}
load(here("modeling_files/all_census_tract_intrinsic.RData")) # for CHD
```

Transforming to Z-scores under Poisson distribution assumption

```{r}
dat_CHD <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 46, 47, 48)]

CHD_resid <- chain1$residuals$response

yhat <- chain1$mean.fitted

CHD_resid_ct <- CHD_resid * fhs_model_df_counts$TotalPopulation / 100

yhat_ct <- yhat * fhs_model_df_counts$TotalPopulation / 100

z <- CHD_resid_ct / sqrt(yhat_ct)
```

```{r}
n <- sum(!is.na(dat_CHD$Data_Value_CHD))

k <- chain1$modelfit[2]


cat ("overdispersion ratio is ", sum(z^2, na.rm = T) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")
```



```{r}
load(here("modeling_files/all_census_tract_BPHIGH.RData"))
```

Transforming to Z-scores under Poisson distribution assumption

```{r}
dat_BPHIGH <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 46, 48)]

BPHIGH_resid <- chain1$residuals$response

yhat <- chain1$mean.fitted

BPHIGH_resid_ct <- BPHIGH_resid * fhs_model_df_counts$TotalPopulation / 100

yhat_ct <- yhat * fhs_model_df_counts$TotalPopulation / 100

z <- BPHIGH_resid_ct / sqrt(yhat_ct)
```



```{r}
n <- sum(!is.na(dat_BPHIGH$Data_Value_BPHIGH))

k <- chain1$modelfit[2]


cat ("overdispersion ratio is ", sum(z^2, na.rm = T) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")
```



```{r}
load(here("modeling_files/all_census_tract_CASTHMA.RData"))
```

Transforming to Z-scores under Poisson distribution assumption

```{r}
dat_CASTHMA <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 47, 48)]

CASTHMA_resid <- chain1$residuals$response

yhat <- chain1$mean.fitted

CASTHMA_resid_ct <- CASTHMA_resid * fhs_model_df_counts$TotalPopulation / 100

yhat_ct <- yhat * fhs_model_df_counts$TotalPopulation / 100

z <- CASTHMA_resid_ct / sqrt(yhat_ct)
```

```{r}
n <- sum(!is.na(dat_CASTHMA$Data_Value_CASTHMA))

k <- chain1$modelfit[2]


cat ("overdispersion ratio is ", sum(z^2, na.rm = T) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")
```

Looks underdispersed in this case.



```{r}
load(here("modeling_files/all_census_tract_MHLTH.RData"))
```

Transforming to Z-scores under Poisson distribution assumption

```{r}
dat_MHLTH <- fhs_model_df_counts[, -c(1, 2, 3, 4:13, 45, 46, 47)]

MHLTH_resid <- chain1$residuals$response

yhat <- chain1$mean.fitted

MHLTH_resid_ct <- MHLTH_resid * fhs_model_df_counts$TotalPopulation / 100

yhat_ct <- yhat * fhs_model_df_counts$TotalPopulation / 100

z <- MHLTH_resid_ct / sqrt(yhat_ct)
```

```{r}
n <- sum(!is.na(dat_MHLTH$Data_Value_MHLTH))

k <- chain1$modelfit[2]


cat ("overdispersion ratio is ", sum(z^2, na.rm = T) / (n-k), "\n")
cat ("p-value of overdispersion test is ", 1 - pchisq (sum(z^2, na.rm = T), n-k), "\n")
```

Looks underdispersed in this case.


